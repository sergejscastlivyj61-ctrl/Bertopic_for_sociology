{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6kaxjneY1w0"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "–¢–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º BERTopic\n",
        "–° –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "\"\"\"\n",
        "\n",
        "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫\n",
        "!pip3 install bertopic\n",
        "!pip3 install plotly\n",
        "!pip3 install wordcloud\n",
        "\n",
        "# –ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bertopic import BERTopic\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "from wordcloud import WordCloud\n",
        "import seaborn as sns\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
        "plt.style.use('default')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
        "df = pd.read_excel('/content/–°–æ–±—ã—Ç–∏—è –Ω–µ–¥–µ–ª–∏_19.xlsx')\n",
        "docs = df['19 –°–û–ë–´–¢–ò–Ø'].tolist()\n",
        "\n",
        "# –ú–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
        "sentence_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã UMAP\n",
        "umap_model = UMAP(\n",
        "    n_neighbors=15,\n",
        "    n_components=30,\n",
        "    min_dist=0.3,\n",
        "    metric='cosine',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã HDBSCAN\n",
        "hdbscan_model = HDBSCAN(\n",
        "    min_cluster_size=5,\n",
        "    min_samples=5,\n",
        "    metric='euclidean',\n",
        "    prediction_data=True\n",
        ")\n",
        "\n",
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ BERTopic\n",
        "topic_model = BERTopic(\n",
        "    embedding_model=sentence_model,\n",
        "    language=\"russian\",\n",
        "    umap_model=umap_model,\n",
        "    hdbscan_model=hdbscan_model,\n",
        "    min_topic_size=8,\n",
        "    n_gram_range=(1, 2),\n",
        "    top_n_words=10,\n",
        "    calculate_probabilities=True\n",
        ")\n",
        "\n",
        "# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
        "topics, probs = topic_model.fit_transform(docs)\n",
        "\n",
        "# –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–µ–º–∞—Ö\n",
        "topic_info = topic_model.get_topic_info()\n",
        "print(\"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–µ–º–∞—Ö:\")\n",
        "print(topic_info)\n",
        "\n",
        "# ============================================================================\n",
        "# –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø 1: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–º (—Å—Ç–æ–ª–±—á–∞—Ç–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞)\n",
        "# ============================================================================\n",
        "\n",
        "def plot_topic_distribution(topics):\n",
        "    \"\"\"–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º.\"\"\"\n",
        "    topic_counts = Counter(topics)\n",
        "\n",
        "    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ ID —Ç–µ–º—ã (–∫—Ä–æ–º–µ -1, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –ø–µ—Ä–≤—ã–º)\n",
        "    sorted_topics = sorted(topic_counts.items(), key=lambda x: (x[0] == -1, x[0]))\n",
        "    topic_ids = [f\"–¢–µ–º–∞ {t[0]}\" for t in sorted_topics]\n",
        "    counts = [t[1] for t in sorted_topics]\n",
        "\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    bars = plt.bar(topic_ids, counts, color='skyblue', edgecolor='navy', alpha=0.7)\n",
        "\n",
        "    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–ø–∏—Å–∏ –∑–Ω–∞—á–µ–Ω–∏–π\n",
        "    for bar, count in zip(bars, counts):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
        "                str(count), ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "    plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º', fontsize=16, fontweight='bold')\n",
        "    plt.xlabel('–¢–µ–º—ã', fontsize=12)\n",
        "    plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤', fontsize=12)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É\n",
        "    total_docs = len(topics)\n",
        "    noise_docs = topic_counts.get(-1, 0)\n",
        "    print(f\"–í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {total_docs}\")\n",
        "    print(f\"–î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —à—É–º–µ (-1): {noise_docs} ({noise_docs/total_docs*100:.1f}%)\")\n",
        "    print(f\"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–º: {len(topic_counts)}\")\n",
        "\n",
        "plot_topic_distribution(topics)\n",
        "\n",
        "# ============================================================================\n",
        "# –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø 2: –û–±–ª–∞–∫–∞ —Å–ª–æ–≤ –¥–ª—è –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ç–µ–º\n",
        "# ============================================================================\n",
        "\n",
        "def plot_wordclouds(topic_model, n_topics=6):\n",
        "    \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ –æ–±–ª–∞–∫–æ–≤ —Å–ª–æ–≤ –¥–ª—è –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ç–µ–º.\"\"\"\n",
        "    # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-N —Ç–µ–º –ø–æ —Ä–∞–∑–º–µ—Ä—É (–∏—Å–∫–ª—é—á–∞—è —à—É–º -1)\n",
        "    topic_sizes = topic_model.get_topic_info().set_index('Topic')['Count'].to_dict()\n",
        "\n",
        "    # –ò—Å–∫–ª—é—á–∞–µ–º —Ç–µ–º—É -1 –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–∞–∑–º–µ—Ä—É\n",
        "    valid_topics = {k: v for k, v in topic_sizes.items() if k != -1}\n",
        "    top_topics = sorted(valid_topics.items(), key=lambda x: x[1], reverse=True)[:n_topics]\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º —Å–µ—Ç–∫—É –≥—Ä–∞—Ñ–∏–∫–æ–≤\n",
        "    n_cols = 3\n",
        "    n_rows = (n_topics + n_cols - 1) // n_cols\n",
        "\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 6*n_rows))\n",
        "    if n_topics == 1:\n",
        "        axes = [axes]\n",
        "    else:\n",
        "        axes = axes.flatten()\n",
        "\n",
        "    for idx, (topic_id, size) in enumerate(top_topics):\n",
        "        if idx >= len(axes):\n",
        "            break\n",
        "\n",
        "        # –ü–æ–ª—É—á–∞–µ–º —Å–ª–æ–≤–∞ —Ç–µ–º—ã\n",
        "        topic_words = topic_model.get_topic(topic_id)\n",
        "        if topic_words:\n",
        "            word_freq = {word: score for word, score in topic_words[:20]}\n",
        "\n",
        "            # –°–æ–∑–¥–∞–µ–º –æ–±–ª–∞–∫–æ —Å–ª–æ–≤\n",
        "            wordcloud = WordCloud(\n",
        "                width=400, height=300,\n",
        "                background_color='white',\n",
        "                colormap='viridis',\n",
        "                max_words=50,\n",
        "                relative_scaling=0.5\n",
        "            ).generate_from_frequencies(word_freq)\n",
        "\n",
        "            axes[idx].imshow(wordcloud, interpolation='bilinear')\n",
        "            axes[idx].set_title(f'–¢–µ–º–∞ {topic_id} (—Ä–∞–∑–º–µ—Ä: {size})', fontsize=14, fontweight='bold')\n",
        "            axes[idx].axis('off')\n",
        "\n",
        "    # –°–∫—Ä—ã–≤–∞–µ–º –ø—É—Å—Ç—ã–µ subplots\n",
        "    for idx in range(len(top_topics), len(axes)):\n",
        "        axes[idx].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_wordclouds(topic_model, n_topics=9)\n",
        "\n",
        "# ============================================================================\n",
        "# –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø 3: –¢–æ–ø-—Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã (–≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã–µ –±–∞—Ä–ø–ª–æ—Ç—ã)\n",
        "# ============================================================================\n",
        "\n",
        "def plot_topic_words(topic_model, n_topics=8, n_words=8):\n",
        "    \"\"\"–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–ø-—Å–ª–æ–≤ –¥–ª—è –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ç–µ–º.\"\"\"\n",
        "    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–º–∞—Ö\n",
        "    topic_info = topic_model.get_topic_info()\n",
        "\n",
        "    # –ò—Å–∫–ª—é—á–∞–µ–º —Ç–µ–º—É -1 –∏ –±–µ—Ä–µ–º —Ç–æ–ø-N —Ç–µ–º\n",
        "    valid_topics = topic_info[topic_info['Topic'] != -1].head(n_topics)\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º —Å–µ—Ç–∫—É –≥—Ä–∞—Ñ–∏–∫–æ–≤\n",
        "    n_cols = 2\n",
        "    n_rows = (n_topics + n_cols - 1) // n_cols\n",
        "\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 6*n_rows))\n",
        "    if n_topics == 1:\n",
        "        axes = [axes]\n",
        "    else:\n",
        "        axes = axes.flatten()\n",
        "\n",
        "    for idx, (_, topic_row) in enumerate(valid_topics.iterrows()):\n",
        "        if idx >= len(axes):\n",
        "            break\n",
        "\n",
        "        topic_id = topic_row['Topic']\n",
        "        topic_size = topic_row['Count']\n",
        "        topic_words = topic_model.get_topic(topic_id)\n",
        "\n",
        "        if topic_words:\n",
        "            words = [word for word, score in topic_words[:n_words]]\n",
        "            scores = [score for word, score in topic_words[:n_words]]\n",
        "\n",
        "            # –ò–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –¥–ª—è –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
        "            y_pos = np.arange(len(words))\n",
        "\n",
        "            axes[idx].barh(y_pos, scores, color='lightcoral', alpha=0.7)\n",
        "            axes[idx].set_yticks(y_pos)\n",
        "            axes[idx].set_yticklabels(words)\n",
        "            axes[idx].set_xlabel('–í–∞–∂–Ω–æ—Å—Ç—å')\n",
        "            axes[idx].set_title(f'–¢–µ–º–∞ {topic_id} (n={topic_size})', fontweight='bold')\n",
        "            axes[idx].grid(axis='x', alpha=0.3)\n",
        "\n",
        "    # –°–∫—Ä—ã–≤–∞–µ–º –ø—É—Å—Ç—ã–µ subplots\n",
        "    for idx in range(len(valid_topics), len(axes)):\n",
        "        axes[idx].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_topic_words(topic_model, n_topics=8)\n",
        "\n",
        "# ============================================================================\n",
        "# –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø 4: Heatmap —Å—Ö–æ–¥—Å—Ç–≤–∞ –º–µ–∂–¥—É —Ç–µ–º–∞–º–∏\n",
        "# ============================================================================\n",
        "\n",
        "def plot_topic_similarity(topic_model):\n",
        "    \"\"\"–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ö–æ–¥—Å—Ç–≤–∞ –º–µ–∂–¥—É —Ç–µ–º–∞–º–∏.\"\"\"\n",
        "    try:\n",
        "        # –ü–æ–ª—É—á–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É —Å—Ö–æ–¥—Å—Ç–≤–∞ —Ç–µ–º\n",
        "        similarity_matrix = topic_model.visualize_topics()\n",
        "\n",
        "        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–±: —Å–æ–∑–¥–∞–µ–º heatmap –≤—Ä—É—á–Ω—É—é\n",
        "        topic_info = topic_model.get_topic_info()\n",
        "        valid_topics = topic_info[topic_info['Topic'] != -1]['Topic'].tolist()[:10]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 10 —Ç–µ–º\n",
        "\n",
        "        if len(valid_topics) > 1:\n",
        "            # –°–æ–∑–¥–∞–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É —Å—Ö–æ–¥—Å—Ç–≤–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—â–∏—Ö —Å–ª–æ–≤\n",
        "            similarity_data = []\n",
        "            for i, topic1 in enumerate(valid_topics):\n",
        "                row = []\n",
        "                words1 = set([word for word, score in topic_model.get_topic(topic1)[:10]])\n",
        "                for j, topic2 in enumerate(valid_topics):\n",
        "                    words2 = set([word for word, score in topic_model.get_topic(topic2)[:10]])\n",
        "                    common_words = len(words1.intersection(words2))\n",
        "                    row.append(common_words)\n",
        "                similarity_data.append(row)\n",
        "\n",
        "            similarity_df = pd.DataFrame(\n",
        "                similarity_data,\n",
        "                index=[f'–¢–µ–º–∞ {t}' for t in valid_topics],\n",
        "                columns=[f'–¢–µ–º–∞ {t}' for t in valid_topics]\n",
        "            )\n",
        "\n",
        "            plt.figure(figsize=(10, 8))\n",
        "            sns.heatmap(similarity_df, annot=True, cmap='YlOrRd', fmt='d',\n",
        "                       cbar_kws={'label': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—â–∏—Ö —Å–ª–æ–≤'})\n",
        "            plt.title('–°—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É —Ç–µ–º–∞–º–∏ (–ø–æ –æ–±—â–∏–º —Å–ª–æ–≤–∞–º)', fontsize=16, fontweight='bold')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç—Ä–æ–∏—Ç—å heatmap —Å—Ö–æ–¥—Å—Ç–≤–∞: {e}\")\n",
        "\n",
        "plot_topic_similarity(topic_model)\n",
        "\n",
        "# ============================================================================\n",
        "# –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø 5: –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã —Ç–µ–º (–µ—Å–ª–∏ –µ—Å—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏)\n",
        "# ============================================================================\n",
        "\n",
        "def plot_topic_timeline(df, topics):\n",
        "    \"\"\"–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞—Ç—ã).\"\"\"\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Å—Ç–æ–ª–±—Ü—ã —Å –¥–∞—Ç–∞–º–∏\n",
        "    date_columns = [col for col in df.columns if '–¥–∞—Ç–∞' in col.lower() or 'date' in col.lower()]\n",
        "\n",
        "    if date_columns:\n",
        "        print(f\"–ù–∞–π–¥–µ–Ω—ã —Å—Ç–æ–ª–±—Ü—ã —Å –¥–∞—Ç–∞–º–∏: {date_columns}\")\n",
        "        # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∫–æ–¥ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ç—Ä–µ–Ω–¥–æ–≤\n",
        "    else:\n",
        "        # –°–æ–∑–¥–∞–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—É—é –≤—Ä–µ–º–µ–Ω–Ω—É—é —à–∫–∞–ª—É –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ —Ç–µ–º–∞–º\n",
        "        topic_docs = {}\n",
        "        for topic, doc in zip(topics, docs):\n",
        "            if topic not in topic_docs:\n",
        "                topic_docs[topic] = []\n",
        "            topic_docs[topic].append(doc)\n",
        "\n",
        "        # –ë–µ—Ä–µ–º —Ç–æ–ø-5 —Ç–µ–º (–∏—Å–∫–ª—é—á–∞—è —à—É–º)\n",
        "        valid_topics = [t for t in topic_docs.keys() if t != -1]\n",
        "        topic_sizes = {t: len(docs) for t, docs in topic_docs.items() if t != -1}\n",
        "        top_topics = sorted(topic_sizes.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "\n",
        "        # –°–æ–∑–¥–∞–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\n",
        "        time_points = np.arange(10)\n",
        "        for topic_id, size in top_topics:\n",
        "            # –ò–º–∏—Ç–∏—Ä—É–µ–º —Ç—Ä–µ–Ω–¥\n",
        "            trend = np.random.normal(size, size*0.2, 10)\n",
        "            plt.plot(time_points, trend, marker='o', linewidth=2, label=f'–¢–µ–º–∞ {topic_id}')\n",
        "\n",
        "        plt.title('–î–∏–Ω–∞–º–∏–∫–∞ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ —Ç–µ–º (–ø—Ä–∏–º–µ—Ä)', fontsize=16, fontweight='bold')\n",
        "        plt.xlabel('–í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ç–æ—á–∫–∏')\n",
        "        plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤')\n",
        "        plt.legend()\n",
        "        plt.grid(alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "plot_topic_timeline(df, topics)\n",
        "\n",
        "# ============================================================================\n",
        "# –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø 6: –†–∞–∑–º–µ—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –≤ –≤–∏–¥–µ –∫—Ä—É–≥–æ–≤–æ–π –¥–∏–∞–≥—Ä–∞–º–º—ã\n",
        "# ============================================================================\n",
        "\n",
        "def plot_topic_pie_chart(topics, top_n=10):\n",
        "    \"\"\"–ö—Ä—É–≥–æ–≤–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º.\"\"\"\n",
        "    topic_counts = Counter(topics)\n",
        "\n",
        "    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–∞–∑–º–µ—Ä—É (–∏—Å–∫–ª—é—á–∞—è —à—É–º)\n",
        "    sorted_counts = sorted([(k, v) for k, v in topic_counts.items() if k != -1],\n",
        "                          key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # –ë–µ—Ä–µ–º —Ç–æ–ø-N —Ç–µ–º, –æ—Å—Ç–∞–ª—å–Ω—ã–µ –æ–±—ä–µ–¥–∏–Ω—è–µ–º –≤ \"–î—Ä—É–≥–∏–µ\"\n",
        "    if len(sorted_counts) > top_n:\n",
        "        main_topics = sorted_counts[:top_n]\n",
        "        other_count = sum(v for k, v in sorted_counts[top_n:])\n",
        "        data = main_topics + [(-2, other_count)]\n",
        "        labels = [f'–¢–µ–º–∞ {k}' for k, v in main_topics] + ['–î—Ä—É–≥–∏–µ —Ç–µ–º—ã']\n",
        "    else:\n",
        "        data = sorted_counts\n",
        "        labels = [f'–¢–µ–º–∞ {k}' for k, v in data]\n",
        "\n",
        "    sizes = [v for k, v in data]\n",
        "\n",
        "    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–º—É —à—É–º–∞ –æ—Ç–¥–µ–ª—å–Ω–æ\n",
        "    noise_count = topic_counts.get(-1, 0)\n",
        "    if noise_count > 0:\n",
        "        sizes.append(noise_count)\n",
        "        labels.append('–®—É–º (-1)')\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    colors = plt.cm.Set3(np.linspace(0, 1, len(sizes)))\n",
        "    wedges, texts, autotexts = plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%',\n",
        "                                      startangle=90, textprops={'fontsize': 10})\n",
        "\n",
        "    # –£–ª—É—á—à–∞–µ–º –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤\n",
        "    for autotext in autotexts:\n",
        "        autotext.set_color('black')\n",
        "        autotext.set_fontweight('bold')\n",
        "\n",
        "    plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º\\n(–ö—Ä—É–≥–æ–≤–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞)',\n",
        "              fontsize=16, fontweight='bold')\n",
        "    plt.axis('equal')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_topic_pie_chart(topics, top_n=8)\n",
        "\n",
        "# ============================================================================\n",
        "# –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø 7: –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ —Å Plotly\n",
        "# ============================================================================\n",
        "\n",
        "def plot_interactive_topics(topic_model, topics):\n",
        "    \"\"\"–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Plotly.\"\"\"\n",
        "    try:\n",
        "        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–º–∞—Ö –¥–ª—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
        "        topic_info = topic_model.get_topic_info()\n",
        "\n",
        "        # –°–æ–∑–¥–∞–µ–º –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –±–∞—Ä–ø–ª–æ—Ç\n",
        "        fig = px.bar(topic_info, x='Topic', y='Count',\n",
        "                     title='–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º (–∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ)',\n",
        "                     labels={'Topic': 'ID —Ç–µ–º—ã', 'Count': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤'},\n",
        "                     color='Count', color_continuous_scale='viridis')\n",
        "\n",
        "        fig.update_layout(\n",
        "            xaxis=dict(tickmode='linear'),\n",
        "            showlegend=False\n",
        "        )\n",
        "        fig.show()\n",
        "\n",
        "        # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è –∫—Ä—É–≥–æ–≤–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞ –¥–ª—è —Ç–æ–ø-10 —Ç–µ–º\n",
        "        top_topics = topic_info[topic_info['Topic'] != -1].head(10)\n",
        "        fig_pie = px.pie(top_topics, values='Count', names='Topic',\n",
        "                        title='–¢–æ–ø-10 —Ç–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤',\n",
        "                        hover_data=['Name'])\n",
        "        fig_pie.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ –Ω–µ –¥–æ—Å—Ç—É–ø–Ω—ã: {e}\")\n",
        "\n",
        "plot_interactive_topics(topic_model, topics)\n",
        "\n",
        "# ============================================================================\n",
        "# –î–ê–õ–ï–ï –ò–î–ï–¢ –í–ê–® –û–†–ò–ì–ò–ù–ê–õ–¨–ù–´–ô –ö–û–î –° –û–¶–ï–ù–ö–û–ô –ö–ê–ß–ï–°–¢–í–ê\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"–ü–ï–†–ï–•–û–î –ö –û–¶–ï–ù–ö–ï –ö–ê–ß–ï–°–¢–í–ê –ú–û–î–ï–õ–ò\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\n",
        "df_after_analiz = pd.DataFrame({\"Document\": docs, \"Topic\": topics})\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫\n",
        "df_events = pd.read_excel('07_23_–°–æ–±—ã—Ç–∏—è_–∫–æ–¥–∏—Ä–æ–≤–∫–∞_–¥–ª—è_–ø—Ä–∏–º–µ—Ä–∞.xlsx', sheet_name='19 –°–û–ë–´–¢–ò–Ø')\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –∫–æ–¥–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞\n",
        "ground_truth = []\n",
        "for idx, row in df_events.iterrows():\n",
        "    codes = []\n",
        "    if not pd.isna(row['–ö–æ–¥1']):\n",
        "        codes.append(row['–ö–æ–¥1'])\n",
        "    if not pd.isna(row['–ö–æ–¥2']):\n",
        "        codes.append(row['–ö–æ–¥2'])\n",
        "    if not pd.isna(row['–ö–æ–¥3']):\n",
        "        codes.append(row['–ö–æ–¥3'])\n",
        "    ground_truth.append(codes)\n",
        "\n",
        "# –†—É—á–Ω–æ–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ (–æ–¥–∏–Ω –∫–æ–¥ –Ω–∞ —Ç–µ–º—É)\n",
        "manual_single_code_mapping = {\n",
        "    -1: 11,      # \"–æ—á–µ–Ω—å_—ç—Ç–æ_—Ç—É—Ä—Ü–∏—è –∑–µ–º–ª–µ—Ç—Ä—è—Å–µ–Ω–∏–µ_–º–æ–±–∏–ª–∏–∑–∞—Ü–∏—é\" ‚Üí –≤—ã–±–∏—Ä–∞–µ–º —Å–ø–µ—Ü–æ–ø–µ—Ä–∞—Ü–∏—é\n",
        "    0: 510,      # \"–∑–æ_–∑–æ –∑o_–∑–æ\" ‚Üí –∑–∞—Ç—Ä—É–¥–Ω—è—é—Å—å –æ—Ç–≤–µ—Ç–∏—Ç—å\n",
        "    # ... –æ—Å—Ç–∞–ª—å–Ω–æ–π –º–∞–ø–ø–∏–Ω–≥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π\n",
        "    1: 510,     2: 250,     3: 11,      4: 11,      5: 510,\n",
        "    6: 11,      7: 250,     8: 11,      9: 11,      10: 510,\n",
        "    11: 11,     12: 250,    13: 510,    14: 150,    15: 11,\n",
        "    16: 60,     17: 380,    18: 11,     19: 250,    20: 250,\n",
        "    21: 510,    22: 250,    23: 11,     24: 510,    25: 11,\n",
        "    26: 510,    27: 11,     28: 510,    29: 510,    30: 250,\n",
        "    31: 510,    32: 40,     33: 250,    34: 510,    35: 510,\n",
        "    36: 510,    37: 16,     38: 510,    39: 11,     40: 11,\n",
        "    41: 11,     42: 40,\n",
        "}\n",
        "\n",
        "# –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∞–ø–ø–∏–Ω–≥ —Å –æ–¥–Ω–∏–º –∫–æ–¥–æ–º\n",
        "predicted_labels_single = []\n",
        "for topic in topics:\n",
        "    predicted_code = manual_single_code_mapping.get(topic, 510)\n",
        "    predicted_labels_single.append([predicted_code])\n",
        "\n",
        "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫\n",
        "def calculate_metrics_single(ground_truth, predicted, code):\n",
        "    tp = 0; fp = 0; fn = 0\n",
        "\n",
        "    for i in range(len(ground_truth)):\n",
        "        gt_has_code = code in ground_truth[i]\n",
        "        pred_has_code = code in predicted[i]\n",
        "\n",
        "        if gt_has_code and pred_has_code:\n",
        "            tp += 1\n",
        "        elif not gt_has_code and pred_has_code:\n",
        "            fp += 1\n",
        "        elif gt_has_code and not pred_has_code:\n",
        "            fn += 1\n",
        "\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    return precision, recall, f1, tp, fp, fn\n",
        "\n",
        "# –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–¥–æ–≤\n",
        "important_codes = [10, 11, 16, 20, 40, 60, 70, 90, 150, 210, 250, 380, 390, 510]\n",
        "\n",
        "results_single = []\n",
        "for code in important_codes:\n",
        "    precision, recall, f1, tp, fp, fn = calculate_metrics_single(ground_truth, predicted_labels_single, code)\n",
        "    results_single.append({\n",
        "        '–ö–æ–¥': code,\n",
        "        'Precision': round(precision, 3),\n",
        "        'Recall': round(recall, 3),\n",
        "        'F1-Score': round(f1, 3),\n",
        "        'TP': tp,\n",
        "        'FP': fp,\n",
        "        'FN': fn\n",
        "    })\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏\n",
        "results_single_df = pd.DataFrame(results_single)\n",
        "print(\"–ú–ï–¢–†–ò–ö–ò –ö–ê–ß–ï–°–¢–í–ê (–û–î–ò–ù –ö–û–î –ù–ê –¢–ï–ú–£):\")\n",
        "print(results_single_df)\n",
        "\n",
        "# –í—ã—á–∏—Å–ª—è–µ–º –º–∏–∫—Ä–æ-—É—Å—Ä–µ–¥–Ω–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏\n",
        "total_tp = sum(r['TP'] for r in results_single)\n",
        "total_fp = sum(r['FP'] for r in results_single)\n",
        "total_fn = sum(r['FN'] for r in results_single)\n",
        "\n",
        "micro_precision_single = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
        "micro_recall_single = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
        "micro_f1_single = 2 * micro_precision_single * micro_recall_single / (micro_precision_single + micro_recall_single) if (micro_precision_single + micro_recall_single) > 0 else 0\n",
        "\n",
        "print(f\"\\n–ú–ò–ö–†–û-–£–°–†–ï–î–ù–ï–ù–ù–´–ï –ú–ï–¢–†–ò–ö–ò (–û–î–ò–ù –ö–û–î –ù–ê –¢–ï–ú–£):\")\n",
        "print(f\"Micro Precision: {micro_precision_single:.3f}\")\n",
        "print(f\"Micro Recall: {micro_recall_single:.3f}\")\n",
        "print(f\"Micro F1-Score: {micro_f1_single:.3f}\")\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞\n",
        "def plot_metrics(results_df):\n",
        "    \"\"\"–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ –∫–æ–¥–∞–º.\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Precision –ø–æ –∫–æ–¥–∞–º\n",
        "    axes[0, 0].bar(results_df['–ö–æ–¥'].astype(str), results_df['Precision'], color='lightblue', alpha=0.7)\n",
        "    axes[0, 0].set_title('Precision –ø–æ –∫–æ–¥–∞–º', fontweight='bold')\n",
        "    axes[0, 0].set_ylabel('Precision')\n",
        "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Recall –ø–æ –∫–æ–¥–∞–º\n",
        "    axes[0, 1].bar(results_df['–ö–æ–¥'].astype(str), results_df['Recall'], color='lightgreen', alpha=0.7)\n",
        "    axes[0, 1].set_title('Recall –ø–æ –∫–æ–¥–∞–º', fontweight='bold')\n",
        "    axes[0, 1].set_ylabel('Recall')\n",
        "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # F1-Score –ø–æ –∫–æ–¥–∞–º\n",
        "    axes[1, 0].bar(results_df['–ö–æ–¥'].astype(str), results_df['F1-Score'], color='lightcoral', alpha=0.7)\n",
        "    axes[1, 0].set_title('F1-Score –ø–æ –∫–æ–¥–∞–º', fontweight='bold')\n",
        "    axes[1, 0].set_ylabel('F1-Score')\n",
        "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ TP/FP/FN –¥–ª—è —Ç–æ–ø-5 –∫–æ–¥–æ–≤ –ø–æ F1\n",
        "    top_codes = results_df.nlargest(5, 'F1-Score')\n",
        "    x = np.arange(len(top_codes))\n",
        "    width = 0.25\n",
        "\n",
        "    axes[1, 1].bar(x - width, top_codes['TP'], width, label='TP', color='green', alpha=0.7)\n",
        "    axes[1, 1].bar(x, top_codes['FP'], width, label='FP', color='red', alpha=0.7)\n",
        "    axes[1, 1].bar(x + width, top_codes['FN'], width, label='FN', color='orange', alpha=0.7)\n",
        "\n",
        "    axes[1, 1].set_title('TP/FP/FN –¥–ª—è —Ç–æ–ø-5 –∫–æ–¥–æ–≤ –ø–æ F1', fontweight='bold')\n",
        "    axes[1, 1].set_xticks(x)\n",
        "    axes[1, 1].set_xticklabels(top_codes['–ö–æ–¥'].astype(str))\n",
        "    axes[1, 1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_metrics(results_single_df)\n",
        "\n",
        "print(f\"\\n‚úÖ –í—Å–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!\")\n",
        "print(f\"üìä –°–æ–∑–¥–∞–Ω–æ 7 —Ç–∏–ø–æ–≤ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è\")"
      ]
    }
  ]
}